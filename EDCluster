#!/usr/bin/env python3
import argparse
import subprocess
from functools import reduce
import os
import pickle

from math import exp
from math import log
from math import sqrt
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans

version = "2.0.0"

citation = "EDCluster version " + version + ".\n" + \
    "Developed by Dominik Schrempf (2020). "

desc = """Scalable detection of empirical distribution mixture models by
transformation and clustering of site distributions. Each amino acid frequency
distribution is considered as a point in twenty dimensional space. The K-Means
clustering algorithm is used to label the points from 1 to K, for a given value
of K. The clusters are output in various file formats, readily usable with
IQ-TREE [1], Phylobayes [2], and RevBayes [3].

Before clustering, coordinate transformations can be applied.

Implemented transformations:
- centered log ratio (CLR) transform [4]
- log centered log ratio (LCLR) transform [5]

Points close to the boundaries, that is, with some coordinates having low
probability, exhibit special features. The coordinate transformations project
these points to a position far away from the projection of points with high
diversity and intermediate frequency values for all coordinates.

The site distribution files are expected to be tab separated, and of the
following form:

----------------------------------------------------------------------
  A C D E F G H I K L M N P Q R S T V W Y

1 0.008 0.001 0.001 0.007 0.002 0.004 0.01 ...
2 ...
...
----------------------------------------------------------------------

"""

usage = """By default, EDcluster calculates mixture model components of empirical
distribution mixture (EDM) models for K={4,8,16,32,64,128,256,512,1024,2048},
and all coordinate transformations. This can take a while. Calculation of an
EDM model with a specific number of components, as well as a specific
transformation can be achieved with the -k, and -t options (see below). """


desc = citation + "\n\n" + desc + "\n\n" + usage

epilog = """

[1] Nguyen, L., Schmidt, H. A., von Haeseler, A., & Minh, B. Q. (2015).
Iq-tree: a fast and effective stochastic algorithm for estimating
maximum-likelihood phylogenies. Molecular Biology and Evolution, 32(1),
268–274. http://dx.doi.org/10.1093/molbev/msu300

[2] Lartillot, N., Rodrigue, N., Stubbs, D., & Richer, J. (2013). Phylobayes
mpi: phylogenetic reconstruction with infinite mixtures of profiles in a
parallel environment. Systematic Biology, 62(4), 611–615.
http://dx.doi.org/10.1093/sysbio/syt022

[3] Höhna, S., Landis, M. J., Heath, T. A., Boussau, B., Lartillot, N., Moore,
B. R., Huelsenbeck, J. P., … (2016). Revbayes: bayesian phylogenetic inference
using graphical models and an interactive model-specification language.
Systematic Biology, 65(4), 726–736. http://dx.doi.org/10.1093/sysbio/syw021

[4] Aitchison, J. (1982). The statistical analysis of compositional data.
Journal of the Royal Statistical Society, Series B (Methological), 44(2),
139–177.

[5] Godichon-Baggioni, A., Maugis-Rabusseau, C., & Rau, A. (2017).
Clustering transformed compositional data using K-means, with applications in
gene expression and bicycle sharing system data. ArXiv, 1–32.

"""


class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter,
                      argparse.RawDescriptionHelpFormatter):
    pass


def parse_arguments():
    parser = argparse.ArgumentParser(
        formatter_class=CustomFormatter,
        description=desc,
        epilog=epilog)
    parser.add_argument("filenames", nargs='+',
                        help="Names of files containing site distributions",
                        default=None)
    parser.add_argument("-t",
                        choices=["none", "clr", "lclr"],
                        help="Use specific transformation.")
    parser.add_argument("-k", type=int,
                        help="Use specific number of clusters.")
    parser.add_argument("-n", action="store_true",
                        help="Also create Nicologos.")
    parser.add_argument('-p', help="Prefix of output files.",
                        default=None)
    parser.add_argument('--pickle-only',
                        help="Read, transform, and save data; then exit.",
                        action="store_true")
    parser.add_argument('--continue-run',
                        help="Use saved data for clustering.",
                        action="store_true")
    parser.add_argument('-v', action="version",
                        version="%(prog)s version " + version)
    return parser.parse_args()


args = parse_arguments()

if args.continue_run and (args.filenames is not None):
    print("WARNING: Saved data will be used for clustering.")
    print("WARNING: Given input files will be ignored:")
    print(args.filenames)
    print("WARNING: If this is not intended, please remove the option [--continue-run].")
elif (not args.continue_run) and (args.filenames is None):
    print("ERROR: No file names given.")
else:
    fns = args.filenames

data_pickle_fn = ".edcluster.data.pickle"
datat_pickle_fn = ".edcluster.datat.pickle"

# By default, use all combinations.
t = args.t
if t is None:
    ts = ["none", "clr", "lclr"]
else:
    ts = [t]

k = args.k
if k is None:
    ks = [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]
else:
    if k > 9999:
        raise ValueError("It is not advised to use so many components. Abort.")
    if k < 2:
        raise ValueError("K has to be larger than 1. Abort.")
    ks = [k]
prefix = args.p

# Definitions.
amino_acids = ["A", "C", "D", "E", "F", "G", "H", "I", "K", "L", "M", "N", "P",
               "Q", "R", "S", "T", "V", "W", "Y"]
n_aa = 20

# Check for Nicologo files.
create_nicologos = args.n
if create_nicologos:
    if not os.path.isfile("modeheader"):
        print("ERROR: File 'modeheader' is missing, " +
              "Nicologo can not be created.")
        raise ValueError()
    elif not os.path.isfile("seqheader"):
        print("ERROR: File 'seqheader' is missing, " +
              "Nicologo can not be created.")
        raise ValueError()
    elif not os.path.isfile("prologo"):
        print("ERROR: Executable 'prologo' is missing, " +
              "Nicologo can not be created.")
        raise ValueError()

# Citation.
print(citation)


def np_array_to_string(a, d=' '):
    """Convert a numpy array to a string."""
    # Generate an array with strings.
    x_arrstr = np.char.mod('%.16f', a)
    # Combine to a string.
    return d.join(x_arrstr)


def read_site_distribution_pd(fn, v=False):
    """Read posterior mean site frequencies from FN using pandas."""
    if v:
        print("Reading file:", fn)
    # Skip header (first two rows).
    data = pd.read_csv(fn, sep="\t", header=None, skiprows=1)
    #  Skip positional information (first column).
    data = data.iloc[:, 1:]
    # Set labels accordingly.
    data.columns = amino_acids
    return data


# Definitions of transformations.
def mean_geometric(a):
    """Geometric mean of array 'a'."""
    product = reduce((lambda x, y: x*y), a)
    n = len(a)
    return product ** (1/n)


def clr(a):
    """Return centered log ratio transformtion of array 'a'."""
    g = mean_geometric(a)
    try:
        return a.apply(lambda x: log(x/g))
    except AttributeError:
        return np.array(list(map(lambda x: log(x/g), a)))


def clr_inv(a):
    """Inverse centered log ratio transform."""
    try:
        es = a.apply(exp)
        g = 1/sum(es)
        return es.apply(lambda x: g*x)
    except AttributeError:
        es = np.array(list(map(exp, a)))
        g = 1.0/np.sum(es)
        return np.array([g*x for x in es])


def clr_df(df):
    return df.apply(clr, axis=1)


def clr_inv_df(df):
    return df.apply(clr_inv, axis=1)


def lclr(a):
    """Log centered log ratio transformation of array-like 'a'."""
    g = mean_geometric(a)

    def f(x):
        if x/g <= 1.0:
            return -(log(1-log(x/g)))**2
        else:
            return log(x/g)**2

    try:
        return a.apply(f)
    except AttributeError:
        return np.array(list(map(f, a)))


def lclr_df(df):
    return df.apply(lclr, axis=1)


def lclr_inv(a):
    """Inverse log centered log ratio transform of array-like 'a'."""

    def f_inv(x):
        """Inverse without normalization."""
        if x <= 0.0:
            return exp(1.0 - exp(sqrt(-x)))
        if x > 0.0:
            return exp(sqrt(x))

    try:
        es = a.apply(f_inv)
        g = 1/sum(es)
        rval = es.apply(lambda x: g*x)
    except AttributeError:
        es = np.array(list(map(f_inv, a)))
        g = 1.0/np.sum(es)
        rval = np.array([g*x for x in es])
    for val in rval:
        if val <= 1e-6:
            print("WARNING: Inverse transformation contains zero.")
            continue
    return rval


def lclr_inv_df(df):
    return df.apply(lclr_inv, axis=1)


# Read data.
if args.continue_run:
    print("Read saved data frame.")
    data = pd.read_pickle(data_pickle_fn)
    print("Done.")
else:
    print("Reading input files.")
    data_array = list(map(read_site_distribution_pd, fns))
    data_conc = pd.concat(data_array, ignore_index=True)
    data = data_conc
    data.to_pickle(data_pickle_fn)
    print("Done.")
    print(len(data), "site distributions.")
    print("")

print("First rows of data:")
print(data.head())
print("")

# Transform data.
if args.continue_run:
    print("Read saved transformed data frame.")
    data_t = pd.read_pickle(datat_pickle_fn)
    print("Done.")
else:
    data_t = {}
    if "none" in ts:
        data_t["none"] = data.copy()
        ts.remove("none")
    if ts != ["none"]:
        print("Transforming data.")
    if "clr" in ts:
        data_t["clr"] = clr_df(data.copy())
        # print("First rows of CLR transformed data:")
        # print(data_t["clr"].head())
        # print("")
        ts.remove("clr")
    if "lclr" in ts:
        data_t["lclr"] = lclr_df(data.copy())
        # print("First rows of LCLR transformed data:")
        # print(data_t["lclr"].head())
        # print("")
        ts.remove("lclr")
    if len(ts) > 0:
        raise ValueError("Transformation(s) not known:", t)
    print("Done.")
    data.to_pickle(datat_pickle_fn)
    print("")

if args.pickle_only:
    print("Pickled data and transformed data. Exit!")
    exit()


def get_weights(ls):
    """Calculate cluster weights by counting points assigned to each label."""
    counts = np.zeros(k, dtype=int)
    for l in ls:
        counts[l] += 1
    s = np.sum(counts)
    return counts/s


def cluster(df, algorithm, args=(), kwds={}):
    cl = algorithm(*args, **kwds)
    labels = cl.fit_predict(df)
    clusters = cl.cluster_centers_
    return (labels, clusters)


# Cluster and output.
print("Clustering.")
print("     K     T")
for k in ks:
    for t in data_t.keys():
        data = data_t[t].copy()
        idx = pd.Index([i for i in range(k)], name="Cluster index")

        cls = pd.Index(["Weight", "Center"])
        clusters_all_lclr = pd.DataFrame([], index=idx, columns=cls)

        print(str(k).rjust(6), t.rjust(6), sep='')
        kwds = {'n_clusters': k, 'n_jobs': -1, 'max_iter': 500, 'tol': 5e-5}
        labels, clusters_raw = cluster(data, KMeans, kwds=kwds)
        clusters = pd.DataFrame(clusters_raw)
        if t == "lclr":
            clusters_freq = clusters.apply(lclr_inv, axis=1)
        elif t == "clr":
            clusters_freq = clusters.apply(clr_inv, axis=1)
        elif t == "none":
            clusters_freq = clusters
        else:
            raise ValueError("Unknown transformation:", t)
        clusters_labels = labels
        clusters_freq = clusters_freq.values.tolist()
        weights = get_weights(clusters_labels)

        if prefix is not None:
            fbn = prefix + "_"
        else:
            fbn = ""
        fbn += "edm_" + str(k).zfill(4) + "_" + t

        clusters_df_unsorted = pd.DataFrame({"Weight": weights,
                                             "Center": clusters_freq})
        clusters_df = clusters_df_unsorted.sort_values(by=["Weight"],
                                                       ascending=False)

        # Phylobayes.
        fn = fbn + "_pb.txt"
        with open(fn, 'x') as fo:
            print("20", ' '.join(amino_acids), file=fo)
            print(k, file=fo)
            for w, c in zip(clusters_df["Weight"], clusters_df["Center"]):
                print(w, np_array_to_string(c), file=fo)

        # Nicologo.
        if create_nicologos:
            fn = fbn + "_nicologo"
            fnt = fn + ".txt"
            with open(fnt, 'x') as fo:
                print(k, n_aa, file=fo)
                i = 0
                for w, c in zip(clusters_df["Weight"], clusters_df["Center"]):
                    print(i, w, np_array_to_string(c), file=fo)
                    i += 1
            nl = "./prologo"
            fnp = fn + ".ps"
            subprocess.call([nl, fnt, fnp])

        # IQ-TREE.
        amino_acids_pb_dict = dict((a, i)
                                   for a, i in
                                   zip(amino_acids, range(n_aa)))
        amino_acids_iqtree = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I',
                              'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']
        amino_acids_iqtree_indices = [amino_acids_pb_dict[a]
                                      for a in amino_acids_iqtree]
        fn = fbn + "_iqtree.nex"
        model_bn = "EDM" + str(k).zfill(4) + t.upper()
        with open(fn, 'x') as fo:
            print("#nexus", file=fo)
            print("begin models;", file=fo)
            i = 0
            class_names = []
            class_names_with_weight = []
            for c, w in zip(clusters_df["Center"], clusters_df["Weight"]):
                class_name = model_bn + "_C" + str(i).zfill(4)
                class_names.append(class_name)
                class_name_with_weight = class_name + ":1.0:" + str(w)
                class_names_with_weight.append(class_name_with_weight)
                # Convert the order to the one used in IQ-TREE.
                c_iqtree = [c[i] for i in amino_acids_iqtree_indices]
                print("frequency ", class_name, " = ",
                      np_array_to_string(c_iqtree), ";", sep='', file=fo)
                i += 1
            print("", file=fo)
            print("frequency ", model_bn, " = FMIX{",
                  ','.join(class_names_with_weight),
                  "};", sep='', file=fo)
            print("frequency ", model_bn, "Opt = FMIX{",
                  ','.join(class_names),
                  "};", sep='', file=fo)
            print("end;", file=fo)

        # RevBayes.
        fn = fbn + "_rb.txt"
        fn_basename_rb = fbn + "_rb"

        options = np.get_printoptions()
        np.set_printoptions(precision=16, threshold=None, linewidth=0)

        with open(fn, 'x') as fo:
            print("# This script is generated automatically.", file=fo)
            print("# Vector of amino acid stationary distributions.", file=fo)
            print("distributions <- [", file=fo)
            idt = "            "
            first = True
            for c in clusters_df["Center"]:
                if first:
                    print(idt, "  ", c, sep='', file=fo)
                    first = False
                else:
                    print(idt, ", ", c, sep='', file=fo)
            print(idt, "]", sep='', file=fo)
            print("# The number of clusters.", file=fo)
            print("n_clusters <- ", k, sep='', file=fo)
            print("# The base file name of this file (for reproducibility).",
                  file=fo)
            print("fn_distributions = \"", fn_basename_rb,
                  "\"", sep='', file=fo)

        np.set_printoptions(options)

if prefix is not None:
    fbn = prefix + "_"
else:
    fbn = ""
fbn += "edm_XXXX_TRNS_EXTENSION"
print("Output written to files of the following form:", fbn)
expl = """where TRNS is the transformation, XXXX is the number of clusters, and EXT is
an extension which depends on the software package. """
print(expl)


print("Done.")
